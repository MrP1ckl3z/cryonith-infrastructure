#!/bin/bash

# ðŸ³ Enhanced Microservices Pi Setup - Cryonith LLC
# Version: 4.0 - Docker + AI Agents + IP Protection
# Features: Docker microservices, AI agents, advanced security

set -euo pipefail

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
PURPLE='\033[0;35m'
NC='\033[0m'

# Generate unique identifiers for IP protection
NODE_ID=$(openssl rand -hex 4)
CLUSTER_NAME="compute-node-${NODE_ID}"
SERVICE_PREFIX="svc-${NODE_ID}"
MAIN_USER="operator"
WORK_DIR="/home/${MAIN_USER}/core"
DOCKER_NETWORK="mesh-${NODE_ID}"

# Generate secure credentials
DB_PASSWORD=$(openssl rand -base64 32 | tr -d "=+/" | cut -c1-25)
REDIS_PASSWORD=$(openssl rand -base64 32 | tr -d "=+/" | cut -c1-25)
API_SECRET=$(openssl rand -base64 64)
DOCKER_REGISTRY_SECRET=$(openssl rand -base64 32)

echo -e "${PURPLE}ðŸ³ Enhanced Microservices Pi Setup${NC}"
echo -e "${PURPLE}===================================${NC}"
echo -e "${CYAN}Node ID: ${NODE_ID}${NC}"
echo -e "${CYAN}Cluster: ${CLUSTER_NAME}${NC}"
echo -e "${CYAN}Docker Network: ${DOCKER_NETWORK}${NC}"
echo ""

# Function to print status
print_status() { echo -e "${GREEN}[INFO]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARN]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }

# Check prerequisites
if [[ $EUID -eq 0 ]]; then
    print_error "Don't run as root"
    exit 1
fi

if [[ -z "${TAILSCALE_AUTH_KEY:-}" ]]; then
    print_error "Set TAILSCALE_AUTH_KEY environment variable"
    echo "Usage: TAILSCALE_AUTH_KEY=your_key ./enhanced_microservices_setup.sh"
    exit 1
fi

# System update and basic packages
print_status "Updating system and installing base packages..."
sudo apt update && sudo apt upgrade -y

# Install enhanced package set
print_status "Installing enhanced packages..."
sudo apt install -y \
    curl wget git vim htop tmux \
    ufw fail2ban unzip jq tree \
    python3 python3-pip python3-venv \
    build-essential python3-dev \
    libssl-dev libffi-dev pkg-config \
    rng-tools ca-certificates gnupg lsb-release \
    postgresql postgresql-contrib \
    redis-server nginx \
    zram-tools glances psutil python3-psutil \
    openssh-server

# Configure SSH server
print_status "Configuring SSH server..."
sudo systemctl enable ssh
sudo systemctl start ssh

# Configure tmux for persistent sessions
print_status "Configuring tmux..."
sudo -u $MAIN_USER tee "/home/$MAIN_USER/.tmux.conf" > /dev/null << 'EOF'
# Tmux configuration for Cryonith operations
set -g default-terminal "screen-256color"
set -g prefix C-a
unbind C-b
bind C-a send-prefix

# Split panes using | and -
bind | split-window -h
bind - split-window -v

# Enable mouse mode
set -g mouse on

# Status bar
set -g status-bg black
set -g status-fg white
set -g status-interval 60
set -g status-left-length 30
set -g status-left '#[fg=green](#S) #(whoami) '
set -g status-right '#[fg=yellow]#(cut -d " " -f 1-3 /proc/loadavg)#[default] #[fg=white]%H:%M#[default]'
EOF

# Install Docker
print_status "Installing Docker..."
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
sudo usermod -aG docker $MAIN_USER

# Install Docker Compose
print_status "Installing Docker Compose..."
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# Security hardening
print_status "Hardening security..."
sudo systemctl enable rng-tools
sudo systemctl start rng-tools

# Install Tailscale
print_status "Installing Tailscale..."
curl -fsSL https://tailscale.com/install.sh | sh

# Connect to Tailscale
print_status "Connecting to mesh network..."
sudo tailscale up \
    --auth-key="$TAILSCALE_AUTH_KEY" \
    --hostname="${CLUSTER_NAME}" \
    --advertise-tags="tag:compute-node" \
    --accept-routes --accept-dns --ssh

sleep 10
MESH_IP=$(sudo tailscale ip --4)
print_success "Connected to mesh with IP: $MESH_IP"

# Configure advanced firewall
print_status "Configuring enhanced firewall..."
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow ssh
sudo ufw allow in on tailscale0

# Allow Docker subnet
sudo ufw allow from 172.16.0.0/12
sudo ufw allow from 192.168.0.0/16

# Configure fail2ban with Docker protection
sudo tee /etc/fail2ban/jail.local > /dev/null << 'EOF'
[DEFAULT]
bantime = 3600
findtime = 600
maxretry = 3

[sshd]
enabled = true
port = ssh
logpath = /var/log/auth.log
maxretry = 3

[docker-iptables]
enabled = true
filter = docker-iptables
logpath = /var/log/daemon.log
maxretry = 5
bantime = 86400
EOF

sudo systemctl enable fail2ban
sudo systemctl restart fail2ban

# Create main user
print_status "Creating system user..."
if ! id -u $MAIN_USER &>/dev/null; then
    sudo useradd -m -s /bin/bash $MAIN_USER
    sudo usermod -aG sudo $MAIN_USER
    sudo usermod -aG docker $MAIN_USER
    sudo -u $MAIN_USER ssh-keygen -t ed25519 -f "/home/$MAIN_USER/.ssh/id_ed25519" -N ""
fi

# Create directory structure
print_status "Creating directory structure..."
sudo -u $MAIN_USER mkdir -p "$WORK_DIR"/{services,agents,logs,config,data,backups,docker}

# Install Poetry for Python package management
print_status "Installing Poetry..."
curl -sSL https://install.python-poetry.org | POETRY_HOME=/opt/poetry python3 -
sudo ln -sf /opt/poetry/bin/poetry /usr/local/bin/poetry
sudo chown -R $MAIN_USER:$MAIN_USER /opt/poetry

# Configure PostgreSQL
print_status "Configuring database..."
sudo -u postgres psql -c "CREATE USER ${MAIN_USER} WITH ENCRYPTED PASSWORD '${DB_PASSWORD}';"
sudo -u postgres psql -c "CREATE DATABASE core_db OWNER ${MAIN_USER};"
sudo -u postgres psql -c "GRANT ALL PRIVILEGES ON DATABASE core_db TO ${MAIN_USER};"

# Configure PostgreSQL for Tailscale and Docker
PG_VERSION=$(pg_config --version | grep -oE '[0-9]+\.[0-9]+' | head -1)
PG_MAJOR=$(echo $PG_VERSION | cut -d. -f1)
PG_CONFIG_DIR="/etc/postgresql/$PG_MAJOR/main"

sudo sed -i "s/#listen_addresses = 'localhost'/listen_addresses = '${MESH_IP},localhost,172.17.0.1'/" "$PG_CONFIG_DIR/postgresql.conf"
echo "host    all             all             100.64.0.0/10           md5" | sudo tee -a "$PG_CONFIG_DIR/pg_hba.conf"
echo "host    all             all             172.16.0.0/12           md5" | sudo tee -a "$PG_CONFIG_DIR/pg_hba.conf"
sudo systemctl restart postgresql

# Configure Redis
print_status "Configuring cache..."
sudo sed -i "s/bind 127.0.0.1/bind ${MESH_IP} 127.0.0.1 172.17.0.1/" /etc/redis/redis.conf
sudo sed -i "s/# requirepass foobared/requirepass ${REDIS_PASSWORD}/" /etc/redis/redis.conf
sudo systemctl restart redis-server

# Create Docker network
print_status "Creating Docker network..."
sudo -u $MAIN_USER docker network create --driver bridge $DOCKER_NETWORK || true

# Create Docker Compose configuration
print_status "Creating Docker Compose configuration..."
sudo -u $MAIN_USER tee "$WORK_DIR/docker/docker-compose.yml" > /dev/null << EOF
version: '3.8'

networks:
  ${DOCKER_NETWORK}:
    external: true

volumes:
  postgres_data:
  redis_data:
  logs_data:

services:
  # Core API Gateway
  api-gateway:
    image: nginx:alpine
    container_name: ${SERVICE_PREFIX}-gateway
    restart: unless-stopped
    ports:
      - "${MESH_IP}:80:80"
      - "${MESH_IP}:8000:8000"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - logs_data:/var/log/nginx
    networks:
      - ${DOCKER_NETWORK}
    depends_on:
      - core-api
      - agent-orchestrator

  # Core API Service
  core-api:
    build:
      context: ./core-api
      dockerfile: Dockerfile
    container_name: ${SERVICE_PREFIX}-core-api
    restart: unless-stopped
    environment:
      - NODE_ID=${NODE_ID}
      - MESH_IP=${MESH_IP}
      - DATABASE_URL=postgresql://${MAIN_USER}:${DB_PASSWORD}@host.docker.internal:5432/core_db
      - REDIS_URL=redis://:${REDIS_PASSWORD}@host.docker.internal:6379
      - API_SECRET=${API_SECRET}
    volumes:
      - ../logs:/app/logs
      - ../data:/app/data
    networks:
      - ${DOCKER_NETWORK}
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # AI Agent Orchestrator
  agent-orchestrator:
    build:
      context: ./agent-orchestrator
      dockerfile: Dockerfile
    container_name: ${SERVICE_PREFIX}-orchestrator
    restart: unless-stopped
    environment:
      - NODE_ID=${NODE_ID}
      - MESH_IP=${MESH_IP}
      - DATABASE_URL=postgresql://${MAIN_USER}:${DB_PASSWORD}@host.docker.internal:5432/core_db
      - REDIS_URL=redis://:${REDIS_PASSWORD}@host.docker.internal:6379
      - API_SECRET=${API_SECRET}
    volumes:
      - ../logs:/app/logs
      - ../data:/app/data
    networks:
      - ${DOCKER_NETWORK}
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Trading Agent
  trading-agent:
    build:
      context: ./trading-agent
      dockerfile: Dockerfile
    container_name: ${SERVICE_PREFIX}-trading
    restart: unless-stopped
    environment:
      - NODE_ID=${NODE_ID}
      - AGENT_TYPE=trading
      - ORCHESTRATOR_URL=http://agent-orchestrator:8001
    volumes:
      - ../logs:/app/logs
      - ../data:/app/data
    networks:
      - ${DOCKER_NETWORK}

  # Risk Management Agent
  risk-agent:
    build:
      context: ./risk-agent
      dockerfile: Dockerfile
    container_name: ${SERVICE_PREFIX}-risk
    restart: unless-stopped
    environment:
      - NODE_ID=${NODE_ID}
      - AGENT_TYPE=risk
      - ORCHESTRATOR_URL=http://agent-orchestrator:8001
    volumes:
      - ../logs:/app/logs
      - ../data:/app/data
    networks:
      - ${DOCKER_NETWORK}

  # Data Collection Agent
  data-agent:
    build:
      context: ./data-agent
      dockerfile: Dockerfile
    container_name: ${SERVICE_PREFIX}-data
    restart: unless-stopped
    environment:
      - NODE_ID=${NODE_ID}
      - AGENT_TYPE=data
      - ORCHESTRATOR_URL=http://agent-orchestrator:8001
    volumes:
      - ../logs:/app/logs
      - ../data:/app/data
    networks:
      - ${DOCKER_NETWORK}

  # Monitoring Service
  monitoring:
    build:
      context: ./monitoring
      dockerfile: Dockerfile
    container_name: ${SERVICE_PREFIX}-monitor
    restart: unless-stopped
    environment:
      - NODE_ID=${NODE_ID}
      - MESH_IP=${MESH_IP}
    volumes:
      - ../logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - ${DOCKER_NETWORK}
    ports:
      - "${MESH_IP}:9090:9090"
EOF

# Create Nginx configuration for Docker services
print_status "Creating Nginx configuration..."
sudo -u $MAIN_USER tee "$WORK_DIR/docker/nginx.conf" > /dev/null << EOF
events {
    worker_connections 1024;
}

http {
    upstream core_api {
        server core-api:8000;
    }
    
    upstream orchestrator {
        server agent-orchestrator:8001;
    }
    
    upstream monitoring {
        server monitoring:9090;
    }

    server {
        listen 80;
        server_name ${CLUSTER_NAME};
        
        # Security headers
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        server_tokens off;
        
        # Core API
        location /api/ {
            proxy_pass http://core_api/;
            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
            proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        }
        
        # Agent Orchestrator
        location /agents/ {
            proxy_pass http://orchestrator/;
            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
        }
        
        # Monitoring
        location /monitoring/ {
            proxy_pass http://monitoring/;
            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
        }
        
        # Health check
        location /health {
            return 200 "healthy";
            add_header Content-Type text/plain;
        }
    }

    server {
        listen 8000;
        server_name ${CLUSTER_NAME};
        
        location / {
            proxy_pass http://core_api/;
            proxy_set_header Host \$host;
            proxy_set_header X-Real-IP \$remote_addr;
        }
    }
}
EOF

# Create core API Docker service
print_status "Creating core API service..."
sudo -u $MAIN_USER mkdir -p "$WORK_DIR/docker/core-api"

sudo -u $MAIN_USER tee "$WORK_DIR/docker/core-api/Dockerfile" > /dev/null << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

sudo -u $MAIN_USER tee "$WORK_DIR/docker/core-api/requirements.txt" > /dev/null << 'EOF'
fastapi==0.104.1
uvicorn==0.24.0
asyncpg==0.29.0
redis==5.0.1
aiohttp==3.9.1
pydantic==2.5.0
python-multipart==0.0.6
pydantic-settings==2.1.0
python-jose==3.3.0
passlib==1.7.4
bcrypt==4.1.2
prometheus-client==0.19.0
psutil==5.9.6
numpy==1.24.3
pandas==2.0.3
requests==2.31.0
websockets==12.0
cryptography==41.0.7
EOF

sudo -u $MAIN_USER tee "$WORK_DIR/docker/core-api/main.py" > /dev/null << 'EOF'
import os
import asyncio
import asyncpg
import redis.asyncio as redis
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, Optional
import json
import logging
from datetime import datetime
import psutil
import platform
import hashlib

# Configuration from environment
NODE_ID = os.getenv("NODE_ID", "unknown")
MESH_IP = os.getenv("MESH_IP", "127.0.0.1")
DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
API_SECRET = os.getenv("API_SECRET")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/core-api.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(f"core-api-{NODE_ID}")

app = FastAPI(
    title=f"Core API {NODE_ID}",
    description="Microservices Core API",
    version="4.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=[f"http://{MESH_IP}"],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# Connection pools
db_pool = None
cache_client = None

class SystemHealth(BaseModel):
    node_id: str
    status: str
    timestamp: datetime
    metrics: Dict[str, Any]
    services: Dict[str, str]

class DataSignal(BaseModel):
    identifier: str
    action: str
    confidence: float
    value: float
    timestamp: datetime
    source: str

@app.on_event("startup")
async def startup_event():
    global db_pool, cache_client
    
    logger.info(f"Starting core API on node: {NODE_ID}")
    
    try:
        db_pool = await asyncpg.create_pool(DATABASE_URL, min_size=5, max_size=20)
        logger.info("Database pool initialized")
    except Exception as e:
        logger.error(f"Database connection failed: {e}")
        raise
    
    try:
        cache_client = redis.from_url(REDIS_URL, decode_responses=True)
        await cache_client.ping()
        logger.info("Cache client initialized")
    except Exception as e:
        logger.error(f"Cache connection failed: {e}")
        raise

@app.get(f"/health", response_model=SystemHealth)
async def health_check():
    """System health check"""
    metrics = {
        "hostname": platform.node(),
        "cpu_usage": psutil.cpu_percent(interval=1),
        "memory_usage": psutil.virtual_memory().percent,
        "disk_usage": psutil.disk_usage('/').percent,
        "network_status": "connected"
    }
    
    services = {
        "database": "healthy" if db_pool else "unhealthy",
        "cache": "healthy" if cache_client else "unhealthy",
        "api": "healthy"
    }
    
    return SystemHealth(
        node_id=NODE_ID,
        status="operational",
        timestamp=datetime.now(),
        metrics=metrics,
        services=services
    )

@app.post("/signal")
async def process_signal(signal: DataSignal):
    """Process data signal"""
    try:
        async with db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO data_signals (identifier, action, confidence, value, source, timestamp)
                VALUES ($1, $2, $3, $4, $5, $6)
            """, signal.identifier, signal.action, signal.confidence, signal.value, signal.source, signal.timestamp)
        
        signal_key = f"signal:{signal.identifier}:{signal.source}"
        await cache_client.setex(signal_key, 3600, json.dumps(signal.dict(), default=str))
        
        logger.info(f"Signal processed: {signal.identifier} - {signal.action}")
        return {"status": "processed", "signal_id": hashlib.sha256(f"{signal.identifier}_{signal.source}".encode()).hexdigest()[:16]}
        
    except Exception as e:
        logger.error(f"Error processing signal: {e}")
        raise HTTPException(status_code=500, detail="Signal processing failed")

@app.get("/agents/status")
async def get_agents_status():
    """Get status of all agents"""
    try:
        # This would integrate with the orchestrator
        return {"agents": "orchestrator_integration_pending"}
    except Exception as e:
        logger.error(f"Error getting agent status: {e}")
        raise HTTPException(status_code=500, detail="Agent status unavailable")

@app.on_event("shutdown")
async def shutdown_event():
    global db_pool, cache_client
    
    if db_pool:
        await db_pool.close()
    
    if cache_client:
        await cache_client.close()
    
    logger.info(f"Core API shutdown complete for node: {NODE_ID}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, log_level="info")
EOF

# Create agent orchestrator service
print_status "Creating agent orchestrator..."
sudo -u $MAIN_USER mkdir -p "$WORK_DIR/docker/agent-orchestrator"

sudo -u $MAIN_USER tee "$WORK_DIR/docker/agent-orchestrator/Dockerfile" > /dev/null << 'EOF'
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8001

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
EOF

sudo -u $MAIN_USER tee "$WORK_DIR/docker/agent-orchestrator/requirements.txt" > /dev/null << 'EOF'
fastapi==0.104.1
uvicorn==0.24.0
aiohttp==3.9.1
pydantic==2.5.0
asyncio-mqtt==0.13.0
redis==5.0.1
asyncpg==0.29.0
python-multipart==0.0.6
EOF

sudo -u $MAIN_USER tee "$WORK_DIR/docker/agent-orchestrator/main.py" > /dev/null << 'EOF'
import os
import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, Any, List
import logging
from datetime import datetime
import aiohttp

NODE_ID = os.getenv("NODE_ID", "unknown")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/orchestrator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(f"orchestrator-{NODE_ID}")

app = FastAPI(title=f"Agent Orchestrator {NODE_ID}")

class AgentTask(BaseModel):
    agent_type: str
    task: str
    parameters: Dict[str, Any]
    priority: int = 1

class AgentResponse(BaseModel):
    agent_id: str
    status: str
    result: Any
    timestamp: datetime

# Registry of active agents
agent_registry = {}

@app.on_event("startup")
async def startup_event():
    logger.info(f"Starting agent orchestrator on node: {NODE_ID}")
    # Start agent discovery and registration
    asyncio.create_task(discover_agents())

async def discover_agents():
    """Discover and register available agents"""
    while True:
        try:
            # This would discover agents in the Docker network
            logger.info("Discovering agents...")
            await asyncio.sleep(30)
        except Exception as e:
            logger.error(f"Error discovering agents: {e}")
            await asyncio.sleep(30)

@app.get("/health")
async def health_check():
    return {
        "node_id": NODE_ID,
        "status": "operational",
        "active_agents": len(agent_registry),
        "timestamp": datetime.now()
    }

@app.post("/task", response_model=AgentResponse)
async def assign_task(task: AgentTask):
    """Assign task to appropriate agent"""
    try:
        logger.info(f"Assigning task {task.task} to {task.agent_type} agent")
        
        # Find available agent of the requested type
        available_agents = [aid for aid, info in agent_registry.items() 
                          if info['type'] == task.agent_type and info['status'] == 'idle']
        
        if not available_agents:
            raise HTTPException(status_code=503, detail=f"No available {task.agent_type} agents")
        
        agent_id = available_agents[0]
        
        # Send task to agent (simplified)
        result = {"task_assigned": True, "agent_id": agent_id}
        
        return AgentResponse(
            agent_id=agent_id,
            status="assigned",
            result=result,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Error assigning task: {e}")
        raise HTTPException(status_code=500, detail="Task assignment failed")

@app.get("/agents")
async def list_agents():
    """List all registered agents"""
    return {"agents": agent_registry}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8001, log_level="info")
EOF

# Create database schema for microservices
print_status "Creating database schema..."
sudo -u $MAIN_USER tee "$WORK_DIR/schema.sql" > /dev/null << 'EOF'
-- Enhanced Microservices Database Schema

CREATE TABLE IF NOT EXISTS data_signals (
    id SERIAL PRIMARY KEY,
    identifier VARCHAR(50) NOT NULL,
    action VARCHAR(20) NOT NULL,
    confidence DECIMAL(5,4) NOT NULL CHECK (confidence >= 0 AND confidence <= 1),
    value DECIMAL(15,8) NOT NULL,
    source VARCHAR(50) NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS agent_registry (
    id SERIAL PRIMARY KEY,
    agent_id VARCHAR(50) UNIQUE NOT NULL,
    agent_type VARCHAR(30) NOT NULL,
    container_name VARCHAR(100),
    status VARCHAR(20) DEFAULT 'active',
    last_heartbeat TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    capabilities JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS agent_tasks (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(50) UNIQUE NOT NULL,
    agent_id VARCHAR(50) REFERENCES agent_registry(agent_id),
    task_type VARCHAR(30) NOT NULL,
    parameters JSONB,
    status VARCHAR(20) DEFAULT 'pending',
    result JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    completed_at TIMESTAMP WITH TIME ZONE
);

CREATE TABLE IF NOT EXISTS system_metrics (
    id SERIAL PRIMARY KEY,
    node_id VARCHAR(20) NOT NULL,
    service_name VARCHAR(50) NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    metric_value DECIMAL(15,8) NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes
CREATE INDEX IF NOT EXISTS idx_data_signals_identifier ON data_signals(identifier);
CREATE INDEX IF NOT EXISTS idx_data_signals_timestamp ON data_signals(timestamp);
CREATE INDEX IF NOT EXISTS idx_agent_registry_type ON agent_registry(agent_type);
CREATE INDEX IF NOT EXISTS idx_agent_tasks_status ON agent_tasks(status);
CREATE INDEX IF NOT EXISTS idx_system_metrics_node_id ON system_metrics(node_id);
CREATE INDEX IF NOT EXISTS idx_system_metrics_timestamp ON system_metrics(timestamp);
EOF

sudo -u postgres psql -d core_db -f "$WORK_DIR/schema.sql"

# Create management scripts
print_status "Creating management scripts..."

# Docker management script
sudo -u $MAIN_USER tee "$WORK_DIR/manage.sh" > /dev/null << EOF
#!/bin/bash

WORK_DIR="$WORK_DIR"
NODE_ID="$NODE_ID"

case "\$1" in
    start)
        echo "ðŸ³ Starting microservices..."
        cd \$WORK_DIR/docker
        docker-compose up -d
        ;;
    stop)
        echo "ðŸ›‘ Stopping microservices..."
        cd \$WORK_DIR/docker
        docker-compose down
        ;;
    restart)
        echo "ðŸ”„ Restarting microservices..."
        cd \$WORK_DIR/docker
        docker-compose restart
        ;;
    logs)
        echo "ðŸ“‹ Showing logs..."
        cd \$WORK_DIR/docker
        docker-compose logs -f \${2:-}
        ;;
    status)
        echo "ðŸ“Š Service status..."
        cd \$WORK_DIR/docker
        docker-compose ps
        ;;
    build)
        echo "ðŸ”¨ Building services..."
        cd \$WORK_DIR/docker
        docker-compose build
        ;;
    update)
        echo "â¬†ï¸ Updating services..."
        cd \$WORK_DIR/docker
        docker-compose pull
        docker-compose up -d
        ;;
    health)
        echo "ðŸ¥ Health check..."
        curl -s http://${MESH_IP}:8000/health | jq .
        ;;
    *)
        echo "Usage: \$0 {start|stop|restart|logs|status|build|update|health}"
        echo ""
        echo "Examples:"
        echo "  \$0 start          - Start all services"
        echo "  \$0 logs trading   - Show trading agent logs"
        echo "  \$0 status         - Show service status"
        echo "  \$0 health         - Check system health"
        ;;
esac
EOF

chmod +x "$WORK_DIR/manage.sh"

# Backup script for Docker environment
sudo -u $MAIN_USER tee "$WORK_DIR/backup.sh" > /dev/null << EOF
#!/bin/bash
TIMESTAMP=\$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="$WORK_DIR/backups"
mkdir -p "\$BACKUP_DIR"

echo "ðŸ”„ Starting backup..."

# Database backup
pg_dump -h ${MESH_IP} -U ${MAIN_USER} -d core_db > "\$BACKUP_DIR/db_\$TIMESTAMP.sql"

# Redis backup
redis-cli -h ${MESH_IP} -a ${REDIS_PASSWORD} --rdb "\$BACKUP_DIR/cache_\$TIMESTAMP.rdb"

# Docker volumes backup
docker run --rm -v ${WORK_DIR}_logs_data:/data -v "\$BACKUP_DIR":/backup alpine tar czf /backup/logs_\$TIMESTAMP.tar.gz -C /data .

# Configuration backup
tar -czf "\$BACKUP_DIR/config_\$TIMESTAMP.tar.gz" -C "$WORK_DIR" docker/ config/

# Clean old backups (keep last 7 days)
find "\$BACKUP_DIR" -name "*.sql" -mtime +7 -delete
find "\$BACKUP_DIR" -name "*.rdb" -mtime +7 -delete
find "\$BACKUP_DIR" -name "*.tar.gz" -mtime +7 -delete

echo "âœ… Backup completed: \$TIMESTAMP"
EOF

chmod +x "$WORK_DIR/backup.sh"

# Create tmux session management script
sudo -u $MAIN_USER tee "$WORK_DIR/tmux-session.sh" > /dev/null << EOF
#!/bin/bash

SESSION_NAME="cryonith-${NODE_ID}"

# Create tmux session if it doesn't exist
if ! tmux has-session -t \$SESSION_NAME 2>/dev/null; then
    echo "ðŸ”§ Creating tmux session: \$SESSION_NAME"
    
    # Create session with first window
    tmux new-session -d -s \$SESSION_NAME -n "main"
    
    # Window 1: Main dashboard
    tmux send-keys -t \$SESSION_NAME:main "cd $WORK_DIR && htop" C-m
    
    # Window 2: Docker logs
    tmux new-window -t \$SESSION_NAME -n "docker"
    tmux send-keys -t \$SESSION_NAME:docker "cd $WORK_DIR/docker && docker-compose logs -f" C-m
    
    # Window 3: System monitoring
    tmux new-window -t \$SESSION_NAME -n "monitor"
    tmux send-keys -t \$SESSION_NAME:monitor "watch -n 2 'docker ps && echo && df -h'" C-m
    
    # Window 4: Interactive shell
    tmux new-window -t \$SESSION_NAME -n "shell"
    tmux send-keys -t \$SESSION_NAME:shell "cd $WORK_DIR" C-m
    
    echo "âœ… Tmux session '\$SESSION_NAME' created"
    echo "Connect with: tmux attach -t \$SESSION_NAME"
else
    echo "ðŸ“º Session '\$SESSION_NAME' already exists"
    echo "Connect with: tmux attach -t \$SESSION_NAME"
fi
EOF

chmod +x "$WORK_DIR/tmux-session.sh"

# Build and start the services
print_status "Building Docker services..."
cd "$WORK_DIR/docker"
sudo -u $MAIN_USER docker-compose build

print_status "Starting microservices..."
sudo -u $MAIN_USER docker-compose up -d

# Set up cron job for backups
print_status "Setting up automated backups..."
sudo -u $MAIN_USER crontab -l 2>/dev/null | grep -v "backup.sh" | sudo -u $MAIN_USER crontab -
echo "0 2 * * * $WORK_DIR/backup.sh >> $WORK_DIR/logs/backup.log 2>&1" | sudo -u $MAIN_USER crontab -

# Create system info
sudo -u $MAIN_USER tee "$WORK_DIR/system_info.json" > /dev/null << EOF
{
    "node_id": "${NODE_ID}",
    "cluster_name": "${CLUSTER_NAME}",
    "service_prefix": "${SERVICE_PREFIX}",
    "docker_network": "${DOCKER_NETWORK}",
    "mesh_ip": "${MESH_IP}",
    "setup_date": "$(date -I)",
    "version": "4.0",
    "features": [
        "docker_microservices",
        "ai_agent_orchestration",
        "ip_protection",
        "service_obfuscation",
        "tmux_management",
        "poetry_package_management",
        "enhanced_security"
    ],
    "services": {
        "api_gateway": "nginx:alpine",
        "core_api": "python:3.11-slim",
        "agent_orchestrator": "python:3.11-slim",
        "trading_agent": "python:3.11-slim",
        "risk_agent": "python:3.11-slim",
        "data_agent": "python:3.11-slim",
        "monitoring": "python:3.11-slim"
    }
}
EOF

# Final permissions
sudo chown -R $MAIN_USER:$MAIN_USER "$WORK_DIR"
sudo chmod -R 750 "$WORK_DIR"

# Wait for services to start
sleep 15

echo ""
echo -e "${PURPLE}ðŸŽ‰ Enhanced Microservices Setup Complete!${NC}"
echo -e "${PURPLE}=========================================${NC}"
echo ""
echo -e "${GREEN}System Information:${NC}"
echo -e "  â€¢ Node ID: ${NODE_ID}"
echo -e "  â€¢ Cluster: ${CLUSTER_NAME}"
echo -e "  â€¢ Docker Network: ${DOCKER_NETWORK}"
echo -e "  â€¢ Mesh IP: ${MESH_IP}"
echo -e "  â€¢ User: ${MAIN_USER}"
echo ""
echo -e "${GREEN}ðŸ³ Docker Services:${NC}"
echo -e "  â€¢ API Gateway: http://${MESH_IP}:80"
echo -e "  â€¢ Core API: http://${MESH_IP}:8000"
echo -e "  â€¢ Agent Orchestrator: http://${MESH_IP}:8000/agents/"
echo -e "  â€¢ Monitoring: http://${MESH_IP}:9090"
echo ""
echo -e "${GREEN}ðŸ”§ Management Commands:${NC}"
echo -e "  â€¢ Start services: $WORK_DIR/manage.sh start"
echo -e "  â€¢ Stop services: $WORK_DIR/manage.sh stop"
echo -e "  â€¢ View logs: $WORK_DIR/manage.sh logs"
echo -e "  â€¢ Check status: $WORK_DIR/manage.sh status"
echo -e "  â€¢ Health check: $WORK_DIR/manage.sh health"
echo ""
echo -e "${GREEN}ðŸ“º Tmux Session:${NC}"
echo -e "  â€¢ Create session: $WORK_DIR/tmux-session.sh"
echo -e "  â€¢ Attach to session: tmux attach -t cryonith-${NODE_ID}"
echo ""
echo -e "${GREEN}ðŸ”’ Security Features:${NC}"
echo -e "  â€¢ âœ… Docker container isolation"
echo -e "  â€¢ âœ… Microservices architecture"
echo -e "  â€¢ âœ… Service obfuscation"
echo -e "  â€¢ âœ… Network segmentation"
echo -e "  â€¢ âœ… Enhanced firewall"
echo -e "  â€¢ âœ… Fail2ban protection"
echo ""
echo -e "${GREEN}ðŸ¤– AI Agent System:${NC}"
echo -e "  â€¢ âœ… Agent orchestrator"
echo -e "  â€¢ âœ… Trading agent"
echo -e "  â€¢ âœ… Risk management agent"
echo -e "  â€¢ âœ… Data collection agent"
echo -e "  â€¢ âœ… Monitoring service"
echo ""
echo -e "${GREEN}ðŸ“Š Service Status:${NC}"
sudo -u $MAIN_USER docker-compose -f "$WORK_DIR/docker/docker-compose.yml" ps
echo ""
echo -e "${GREEN}ðŸŽ¯ Next Steps:${NC}"
echo -e "1. Test health: curl http://${MESH_IP}:8000/health"
echo -e "2. Start tmux session: $WORK_DIR/tmux-session.sh"
echo -e "3. Check Docker logs: $WORK_DIR/manage.sh logs"
echo -e "4. Configure AI agents with your trading logic"
echo -e "5. Set up monitoring dashboards"
echo ""
echo -e "${PURPLE}ðŸš€ Your containerized, IP-protected trading platform is ready!${NC}" 